{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import os\n",
    "import regex as re\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclean = pd.read_csv(\n",
    "    'artifacts\\\\evaluations_first_run.csv', index_col=None\n",
    ")\n",
    "\n",
    "unclean_before = unclean.iloc[:1125]\n",
    "unclean_after = unclean.iloc[1125:]\n",
    "\n",
    "unclean_before = unclean_before.drop(unclean_before.columns[0], axis=1)\n",
    "unclean_before = unclean_before.drop(unclean_before.columns[-1], axis=1)\n",
    "\n",
    "unclean_after = unclean_after.drop(unclean_after.columns[-2:], axis=1)\n",
    "unclean_after.columns = unclean_before.columns\n",
    "\n",
    "cleaned = pd.concat(\n",
    "    [\n",
    "        unclean_before, \n",
    "        unclean_after\n",
    "    ]\n",
    ")\n",
    "cleaned.to_csv(\n",
    "    \"artifacts/all_runs_cleaned.csv\", index=None\n",
    ")\n",
    "all_runs = pd.read_csv(\n",
    "    \"artifacts/all_runs_cleaned.csv\"\n",
    ")\n",
    "\n",
    "all_runs[\"Timeout\"] = pd.to_numeric(all_runs[\"Timeout\"], errors='coerce')\n",
    "all_runs[\"StartSol_Profit\"] = pd.to_numeric(all_runs[\"StartSol_Profit\"], errors='coerce')\n",
    "all_runs[\"MH_Profit\"] = pd.to_numeric(all_runs[\"MH_Profit\"], errors='coerce')\n",
    "all_runs[\"Runtime\"] = pd.to_numeric(all_runs[\"Runtime\"], errors='coerce')\n",
    "all_runs[\"Improvement\"] = all_runs[\"MH_Profit\"] - all_runs[\"StartSol_Profit\"]\n",
    "\n",
    "all_runs[\"AlgoConfig\"] = all_runs.AlgoConfig.apply(lambda x: x.replace(\"array\",\"\").replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "all_runs['AlgoConfig_parsed'] = all_runs['AlgoConfig'].apply(ast.literal_eval)\n",
    "normAC = pd.json_normalize(all_runs[\"AlgoConfig_parsed\"]).add_prefix(\"AlgoConfig.\")\n",
    "all_runs = all_runs.join(normAC)\n",
    "\n",
    "all_runs[\"Parameters\"] = all_runs.Parameters.apply(lambda x: x.replace(\"array\",\"\").replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "all_runs['Parameters_parsed'] = all_runs['Parameters'].apply(ast.literal_eval)\n",
    "normAC = pd.json_normalize(all_runs[\"Parameters_parsed\"]).add_prefix(\"Parameters.\")\n",
    "all_runs = all_runs.join(normAC)\n",
    "\n",
    "all_runs = all_runs.loc[all_runs[\"AlgoConfig.n\"] != 6] ### Remove example.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100  10  20  50] [0.666] [4] [2]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    all_runs.iloc[:, 9].unique(),\n",
    "    all_runs.iloc[:, 10].unique(),\n",
    "    all_runs.iloc[:, 11].unique(),\n",
    "    all_runs.iloc[:, 12].unique(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_names = {\n",
    "    \"lns\" : \"LNS\",\n",
    "    \"simulated_annealing\" : \"SA\",\n",
    "    \"large_neighborhood_search_simulated_annealing\" : \"LNS+SA\",\n",
    "    \"tabu_search\" : \"TS\",\n",
    "    \"tabu_search_lns\" : \"TS+LNS\",\n",
    "    \"lns_ts_simulated_annealing\" : \"LNS+TS+SA\",\n",
    "    \"reactive_tabu_search\" : \"RTS\",\n",
    "    # \"lns_gc\" : \"LNS+GC\"\n",
    "}\n",
    "\n",
    "\n",
    "# @article{gramazio-2017-ccd,\n",
    "#   author={Gramazio, Connor C. and Laidlaw, David H. and Schloss, Karen B.},\n",
    "#   journal={IEEE Transactions on Visualization and Computer Graphics},\n",
    "#   title={Colorgorical: creating discriminable and preferable color palettes for information visualization},\n",
    "#   year={2017}\n",
    "# }\n",
    "mh_colors = {\n",
    "    \"lns\" : \"#256676\", \n",
    "    \"simulated_annealing\" : \"#63ef85\", \n",
    "    \"large_neighborhood_search_simulated_annealing\" : \"#eb1241\", \n",
    "    \"tabu_search\" : \"#20d8fd\", \n",
    "    \"tabu_search_lns\" : \"#9c3190\", \n",
    "    \"lns_ts_simulated_annealing\" : \"#afc6fe\",\n",
    "    \"reactive_tabu_search\" : \"#4d57a8\", \n",
    "    # \"lns_gc\" : \"#ffa8ff\",   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_overall_configuration(data, meta_name, decision_argument):\n",
    "    assert meta_name in mh_names.keys(), \"This metaheuristic isn't available, yet\"\n",
    "    assert decision_argument in [\"MH_Profit\", \"Runtime\"], \"You can only decide by MH_Profit or Runtime\"\n",
    "\n",
    "\n",
    "    algorithm_data = data.loc[data.MetaName == meta_name].copy()            # Filter for the specified algorithm\n",
    "    algorithm_data['Parameters'] = algorithm_data['Parameters'].apply(ast.literal_eval)         # Parse the Parameters column to extract parameter values\n",
    "    algorithm_data['Parameters_tuple'] = algorithm_data['Parameters'].apply(lambda x: tuple(sorted(x.items()))) # Convert the Parameters column from dictionaries to frozensets for hashing\n",
    "\n",
    "    # Count the frequency of each configuration achieving the best profit\n",
    "    if decision_argument == \"MH_Profit\":\n",
    "        algorithm_data['best'] = algorithm_data.groupby('AlgoConfig')[f'{decision_argument}'].transform('max') == algorithm_data[f'{decision_argument}']\n",
    "    if decision_argument == \"Runtime\":\n",
    "        algorithm_data['best'] = algorithm_data.groupby('AlgoConfig')[f'{decision_argument}'].transform('min') == algorithm_data[f'{decision_argument}']\n",
    "    counts = algorithm_data[algorithm_data['best']].groupby('Parameters_tuple').size().reset_index(name='count')\n",
    "\n",
    "    best = counts.loc[counts['count'].idxmax()]     # Identify the best configuration based on the frequency\n",
    "\n",
    "    return dict(best['Parameters_tuple'])\n",
    "\n",
    "def pd_inner_json(ccc):\n",
    "    return tuple(sorted(ast.literal_eval(ccc).items()))\n",
    "\n",
    "def plot_comparison(data, meta1, meta2, decision_variable=\"MH_Profit\"):\n",
    "    assert (meta1 != meta2), \"You can only compare different metaheuritics\"\n",
    "    assert (meta1 in mh_names.keys()) and (meta2 in mh_names.keys()), f\"At least one of your metaheuristics is not in {mh_names.keys()}\"\n",
    "\n",
    "    output_dir = 'evaluation_plots/PerformaceProfiles'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    conf1 = find_best_overall_configuration(data, meta1, decision_variable)\n",
    "    conf2 = find_best_overall_configuration(data, meta2, decision_variable)    \n",
    "    \n",
    "    runs1 = data.loc[\n",
    "        (data.MetaName == meta1) & \n",
    "        (data['Parameters'].apply(pd_inner_json) == tuple(sorted(conf1.items())))\n",
    "    ].copy()\n",
    "    runs2 = data.loc[\n",
    "        (data.MetaName == meta2) & \n",
    "        (data['Parameters'].apply(pd_inner_json) == tuple(sorted(conf2.items())))\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "    runs1.sort_values(by=[\"AlgoConfig\"], inplace=True)\n",
    "    runs2.sort_values(by=[\"AlgoConfig\"], inplace=True)\n",
    "\n",
    "    unique_colors = runs1['AlgoConfig.n'].unique()\n",
    "    unique_colors = sorted(unique_colors)\n",
    "    color_map = {val: idx for idx, val in enumerate(unique_colors)}\n",
    "    colors1 = runs1['AlgoConfig.n'].map(color_map)\n",
    "    colors2 = runs2['AlgoConfig.n'].map(color_map)\n",
    "    palette = plt.get_cmap(\"tab20b\", len(unique_colors))\n",
    "\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 6))\n",
    "\n",
    "    for ii, var in enumerate([\"Improvement\", \"Runtime\"]):\n",
    "        ax_min = np.minimum(runs1[var].min(), runs2[var].min())\n",
    "        ax_max = np.maximum(runs1[var].max(), runs2[var].max())\n",
    "\n",
    "        scatter = axes[ii].scatter(runs1[var], runs2[var], c=colors1, cmap=palette, marker='x')\n",
    "        axes[ii].plot(\n",
    "            [ax_min, ax_max],\n",
    "            [ax_min, ax_max], \n",
    "            'r--'\n",
    "        )\n",
    "        axes[ii].set_xlabel(f'{var} ({mh_names[meta1]})', fontsize=9)\n",
    "        axes[ii].set_ylabel(f'{var} ({mh_names[meta2]})', fontsize=9)\n",
    "        axes[ii].set_title(f'Compare ({mh_names[meta1]} vs {mh_names[meta2]})', fontsize=11)\n",
    "        axes[ii].grid(True)\n",
    "\n",
    "    handles, labels = scatter.legend_elements(prop=\"colors\")\n",
    "    legend_labels = [f\"{label}\" for label in unique_colors]\n",
    "    axes[1].legend(handles, legend_labels, title=\"#Teams\", bbox_to_anchor=(1.25, 1.02))\n",
    "\n",
    "    plot_filename = os.path.join(output_dir, f'pps_{mh_names[meta1]}_{mh_names[meta2]}.png')\n",
    "    plt.savefig(plot_filename, dpi=300, format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "for m1, m2 in list(combinations(mh_names.keys(), 2)):\n",
    "    plot_comparison(all_runs, m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplots_per_n(data, decision_variable=\"MH_Profit\"):\n",
    "    output_dir = 'evaluation_plots/Boxplots'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    unique_n_values = sorted(data['AlgoConfig.n'].unique())\n",
    "    df_best_confs = None #pd.DataFrame(columns=data.columns)\n",
    "    for meta in mh_names.keys():\n",
    "        conf = find_best_overall_configuration(data, meta, decision_variable)    \n",
    "        runs_meta = data.loc[\n",
    "            (data.MetaName == meta) & \n",
    "            (data['Parameters'].apply(pd_inner_json) == tuple(sorted(conf.items())))\n",
    "        ].copy()\n",
    "        df_best_confs = pd.concat(\n",
    "            [df_best_confs, runs_meta]\n",
    "        )\n",
    "\n",
    "\n",
    "    # Für jeden einzigartigen Wert von AlgoConfig.n eine eigene Grafik erstellen und speichern\n",
    "    for n_value in unique_n_values:\n",
    "\n",
    "        plt.figure(figsize=(9, 6))\n",
    "        ax = sns.boxplot(x='AlgoConfig.n', y='Improvement', hue='MetaName',  data=df_best_confs[df_best_confs['AlgoConfig.n'] == n_value], showfliers=False, width=0.8, palette=mh_colors)\n",
    "        plt.title(f'Boxplot of improvement by metaheuristic', fontsize=14)\n",
    "        plt.xlabel('')\n",
    "        plt.xticks(ticks=[], labels=[])\n",
    "        plt.ylabel('Improvement', fontsize=11)\n",
    "        plt.grid(axis='y')\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        labels = [mh_names[label] for label in labels]\n",
    "        plt.legend(handles=handles, labels=labels, title='Metaheuristic', bbox_to_anchor=(1.25, 1.02))\n",
    "        \n",
    "\n",
    "        # Speichern der Grafik\n",
    "        plot_filename = os.path.join(output_dir, f'boxplot_AlgoConfig_n_{n_value}.png')\n",
    "        plt.savefig(plot_filename, dpi=300, format=\"png\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        # plt.show()\n",
    "\n",
    "boxplots_per_n(all_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[ht]\n",
      "\\centering\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "Meta Name & Min & Max & Q25 & Q50 & Q75 \\\\\n",
      "\\midrule\n",
      "LNS & 0.7 & 2.3 & 0.8 & 0.9 & 1.2 \\\\\n",
      "SA & 0.5 & 3.8 & 0.9 & 1.3 & 2.0 \\\\\n",
      "TS & 0.7 & 2.5 & 0.9 & 1.1 & 1.4 \\\\\n",
      "LNS+SA & 29.8 & 29.9 & 29.8 & 29.8 & 29.8 \\\\\n",
      "TS+LNS & 0.5 & \\textbf{1.5} & \\textbf{0.7} & \\textbf{0.8} & \\textbf{1.0} \\\\\n",
      "LNS+TS+SA & 29.8 & 29.9 & 29.8 & 29.8 & 29.8 \\\\\n",
      "RTS & \\textbf{0.4} & 3.2 & 0.9 & 1.1 & 1.3 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Results of Runtime with 10 teams}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "variable = \"Runtime\"\n",
    "n = 10\n",
    "\n",
    "result = all_runs.loc[\n",
    "    all_runs[\"AlgoConfig.n\"] == n\n",
    "].groupby(\n",
    "    [\"MetaName\", \"AlgoConfig.n\"]\n",
    ")[[variable]].agg(\n",
    "    Min=(variable, lambda x: x.min()),\n",
    "    Max=(variable, lambda x: x.max()),\n",
    "    Q25=(variable, lambda x: x.quantile(0.25)),\n",
    "    Q50=(variable, lambda x: x.quantile(0.50)),\n",
    "    Q75=(variable, lambda x: x.quantile(0.75)),   \n",
    ").copy()\n",
    "\n",
    "result = result.rename_axis(index={'MetaName': 'Meta Name', 'AlgoConfig.n': 'n'}).reset_index()\n",
    "result[\"Meta Name\"] = result[\"Meta Name\"].replace(mh_names)\n",
    "\n",
    "sort_order = [\"LNS\",\"SA\",\"TS\",\"LNS+SA\",\"TS+LNS\",\"LNS+TS+SA\",\"RTS\"]\n",
    "result['Meta Name'] = pd.Categorical(result['Meta Name'], categories=sort_order, ordered=True)\n",
    "result = result.sort_values('Meta Name')\n",
    "\n",
    "cols = [col for col in result.columns if col != \"n\"]\n",
    "result = result[cols]\n",
    "\n",
    "# Funktion zum Markieren der größten Werte\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.min()\n",
    "    return ['\\\\textbf{' + '{:.1f}'.format(v) + '}' if m else '{:.1f}'.format(v) for v, m in zip(s, is_max)]\n",
    "\n",
    "# Markiere die größten Werte in den relevanten Spalten\n",
    "result['Min'] = highlight_max(result['Min'])\n",
    "result['Max'] = highlight_max(result['Max'])\n",
    "result['Q25'] = highlight_max(result['Q25'])\n",
    "result['Q50'] = highlight_max(result['Q50'])\n",
    "result['Q75'] = highlight_max(result['Q75'])\n",
    "\n",
    "\n",
    "latex_table = result.to_latex(\n",
    "        index=False,\n",
    "        formatters={\"Meta Name\": str.upper},\n",
    "        float_format=\"{:.1f}\".format,\n",
    ")\n",
    "\n",
    "latex_table_with_title = r\"\"\"\\begin{table}[ht]\n",
    "\\centering\n",
    "\"\"\" + latex_table + r\"\"\"\\caption{Results of \"\"\" + variable + f\" with {n} teams\" + \"}\\n\" + r\"\\end{table}\"\n",
    "\n",
    "print(latex_table_with_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vielleicht noch nützlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m decision_variable\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMH_Profit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mh \u001b[38;5;129;01min\u001b[39;00m mh_names\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      5\u001b[0m     conf \u001b[38;5;241m=\u001b[39m find_best_overall_configuration(test, mh, decision_variable)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "decision_variable=\"MH_Profit\"\n",
    "data = test.copy()\n",
    "\n",
    "for mh in mh_names.keys():\n",
    "    conf = find_best_overall_configuration(test, mh, decision_variable)\n",
    "    runs = data.loc[\n",
    "        (data.MetaName == mh) & \n",
    "        (data['Parameters'].apply(get_configs) == tuple(sorted(conf.items())))\n",
    "    ].copy()\n",
    "\n",
    "    fig, axes = plt.subplots(1,1, figsize=(10, 5))\n",
    "    step = axes.step(runs.Runtime, runs.Improvement)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
